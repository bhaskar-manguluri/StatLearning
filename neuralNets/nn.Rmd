---
title: "NeuralNets"
author: "bhaskar"
output: html_document
---

 

### Projection pursuit regression and neural networks
The central concept in both methods to take linear combination of input features and then model target as non linear function of these derived features

project pirsuit regression f(x) = sum(g(transpose(w)*X))

g is the non linear function and the name is because we are in a pirsuit of projection along the direction defined by unit vector matrix which explains the data well.

### neural networks 

instead of using a non parametric scatter plot smoother as in project pirsuit regression , neural nets uses predefined activation function like sigmoid transformation to iteratively(hidden layers) derive the non linear transformations and model a linear model in hidden units

- Z (transformed scores) = activationFunction(linear combination of inputs)
- T = linear combination of Z
- f(X) = g(T) , where g is ouput function .
- linear combination of inputs from one layer can use constant intercept(bias constant)
- general activation function is *sigmoid transformation*, where any other function like *gaussian radial basis* or *tanh()*  can be used, with condition as non decreasing , bounded and differentiable function
- general output function in case of regression is identity function, with one output making it a linear model in hidden units, where as for  classification it is sofmax function, which gives postive values for each class which sum up to one as in muti class logistic regression
- Neural network with one hidden layer is almost same as PPR , where output function for PPR is non parametric function where in case of neural networks it is a simple function based on activation function

**Fitting neural networks**

 Neural network is a parameteric approach with unknown parameters also called as weights, error function is a function of weights, and are calculated by minimising the error function, typical error function for regression setting is *least squares* and for classification it is *cross entropy*. To avoid overfitting traditionally early stopping the training procedure, adding a regularising term(also known as weight decay) can also be employed and weight decay is found to have substancial improvement in the predictions of neural networks. Generally error function is minimised by gradient descent, called backward propagation in this setting as the structure of the problem makes it easier to calculate gradient using chain rule of differentiation. learning rate and number of iterations determine the convergence of gradient descent. Backward propagation can be very slow alternate better fitting appraches are conjugate gradients and variable metric methods.Package **neural net** has methods for resilient backprogation and globally convergent version introduced by Anastasiadis et al. (2005).

### Practical Issues with training NN

Neural network is generally overparametrized, hence certain guidelines may be useful in fitting

1. **Starting values**
- If weights are small neural networks are close to zero, then the neural network from sigmoid activation is close to linear model. 
- Initial weights are chosen to be random values near zero and it becomes non linear as weights increase
- Starting instead with large weights often leads to poor solutions

2. **Over fitting**
- prone to overfit hence add penalty to error function controlled by *weight decay* variable as tuning parameter, and use CV to estimate it. 

3. Scaling of the inputs
- Scaling is preferred to treat all the bottom layers equally

4. Number of hidden units and layers
- with too few hidden units the model might not have enough flexibility to capture nonlinearities
- if appropriate regularisation param is selected with too many hidden units then overfitting can be avoided
- multiple hidden layers allow construction of hierarchial features at different levels of resolution

5. Multiple Minima
 There can be many local minima, making the output dependent on starting weights, so trying out with different random starting weights configuration can be helpful

Bayesion approach to fitting NN bayesian Neural networks: using MCMC is shown to improve nn accuracy

Boosted neural Networks : boosting is efficient with weak learners and hence too few hidden layers(generally 1) and few units are used .

##Implementing neural net

*** Data Set generation***
```{r}
library(ggplot2)
library(caret)

# generate data for testing the implemented neural network
N <- 200 # number of points per class
D <- 2 # dimensionality
K <- 4 # number of classes
X <- data.frame() # data matrix (each row = single example)
y <- data.frame() # class labels

set.seed(308)

for (j in (1:K)){
  r <- seq(0.05,1,length.out = N) # radius
  t <- seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp <- data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp <- data.frame(matrix(j, N, 1))
  X <- rbind(X, Xtemp)
  y <- rbind(y, ytemp)
}

data <- cbind(X,y)
colnames(data) <- c(colnames(X), 'label')
```
X, y are 800 by 2 and 800 by 1 data frames respectively, and they are created in a way such that a linear classifier cannot separate them. They are roughly evenly spaced and a line is not a good decision boundary

***Neural networks construction***
Now, let’s construct a NN with 2 layers. But before that, we need to convert X into a matrix (for matrix operation later on). For labels in y, a new matrix Y (800 by 4) is created such that for each example (each row in Y), the entry with index==label is 1 (and 0 otherwise).
```{r}
X <- as.matrix(X)
Y <- matrix(0, N*K, K)

for (i in 1:(N*K)){
  Y[i, y[i,1]] <- 1
}
```
Next, let’s build a function ‘nnet’ that takes two matrices X and Y and returns a list of 4 with W, b and W2, b2 (weight and bias for each layer). I can specify step_size (learning rate) and regularization strength (reg, sometimes symbled as lambda).

For the choice of activation and loss (cost) function, ReLU and softmax are selected respectively.Note that the implementation below uses vectorized operation that may seem hard to follow. If so, you can write down dimensions of each matrix and check multiplications and so on. By doing so, you also know what’s under the hood for a neural network.

```{r}
nnet <- function(X, Y, step_size = 0.5, reg = 0.001, h = 10, niteration){
  # get dim of input
  N <- nrow(X) # number of examples
  K <- ncol(Y) # number of classes
  D <- ncol(X) # dimensionality
  
  # initialize parameters randomly
  W <- 0.01 * matrix(rnorm(D*h), nrow = D)
  b <- matrix(0, nrow = 1, ncol = h)
  W2 <- 0.01 * matrix(rnorm(h*K), nrow = h)
  b2 <- matrix(0, nrow = 1, ncol = K)
  
  # gradient descent loop to update weight and bias
  for (i in 0:niteration){
    # hidden layer, ReLU activation
    hidden_layer <- pmax(0, X%*% W + matrix(rep(b,N), nrow = N, byrow = T))
    hidden_layer <- matrix(hidden_layer, nrow = N)
    # class score
    scores <- hidden_layer%*%W2 + matrix(rep(b2,N), nrow = N, byrow = T)
    
    # compute and normalize class probabilities(softmax)
    exp_scores <- exp(scores)
    probs <- exp_scores / rowSums(exp_scores)
    
    # compute the loss: sofmax and regularization
    corect_logprobs <- -log(probs)
    data_loss <- sum(corect_logprobs*Y)/N
    reg_loss <- 0.5*reg*sum(W*W) + 0.5*reg*sum(W2*W2)
    loss <- data_loss + reg_loss
    
    # check progress
    if (i%%1000 == 0 | i == niteration){
      print(paste("iteration", i,': loss', loss))}
    
    # compute the gradient on scores
    dscores <- probs-Y
    dscores <- dscores/N
    
    # backpropate the gradient to the parameters
    dW2 <- t(hidden_layer)%*%dscores
    db2 <- colSums(dscores)
    # next backprop into hidden layer
    dhidden <- dscores%*%t(W2)
    # backprop the ReLU non-linearity
    dhidden[hidden_layer <= 0] <- 0
    # finally into W,b
    dW <- t(X)%*%dhidden
    db <- colSums(dhidden)
    
    # add regularization gradient contribution
    dW2 <- dW2 + reg *W2
    dW <- dW + reg *W
    
    # update parameter 
    W <- W-step_size*dW
    b <- b-step_size*db
    W2 <- W2-step_size*dW2
    b2 <- b2-step_size*db2
  }
  return(list(W, b, W2, b2))
}
```
*** prediction function and model training***
```{r}
nnetPred <- function(X, para = list()){
  W <- para[[1]]
  b <- para[[2]]
  W2 <- para[[3]]
  b2 <- para[[4]]
  
  N <- nrow(X)
  hidden_layer <- pmax(0, X%*% W + matrix(rep(b,N), nrow = N, byrow = T)) 
  hidden_layer <- matrix(hidden_layer, nrow = N)
  scores <- hidden_layer%*%W2 + matrix(rep(b2,N), nrow = N, byrow = T) 
  predicted_class <- apply(scores, 1, which.max)

  return(predicted_class)  
}

nnet.model <- nnet(X, Y, step_size = 0.4,reg = 0.0002, h=50, niteration = 7000)
predicted_class <- nnetPred(X, nnet.model)
print(paste('training accuracy:',mean(predicted_class == (y))))
```
*** Decsion Boundary***
Next, let’s plot the decision boundary. We can also use the caret package and train different classifiers with the data and visualize the decision boundaries. It is very interesting to see how different algorithms make decisions.(add comparison with caret and others to RMd)
```{r}
# plot the resulting classifier
hs <- 0.01
x_min <- min(X[,1])-0.2; x_max <- max(X[,1])+0.2
y_min <- min(X[,2])-0.2; y_max <- max(X[,2])+0.2
grid <- as.matrix(expand.grid(seq(x_min, x_max, by = hs), seq(y_min, y_max, by =hs)))
Z <- nnetPred(grid, nnet.model)

ggplot()+
  geom_tile(aes(x = grid[,1],y = grid[,2],fill=as.character(Z)), alpha = 0.3, show.legend = F)+ 
  geom_point(data = data, aes(x=x, y=y, color = as.character(label)), size = 2) +
  xlab('X') + ylab('Y') + ggtitle('Neural Network Decision Boundary') +
  scale_color_discrete(name = 'Label') + coord_fixed(ratio = 0.6) +
  theme(axis.ticks=element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        axis.text=element_blank(),legend.position = 'none')
```
Using  the above neural network on MNSIT data from kaggle
```{r}
displayDigit <- function(X){
  m <- matrix(unlist(X),nrow = 28,byrow = T)
  m <- t(apply(m, 2, rev))
  image(m,col=grey.colors(255))
}

train <- read.csv("train.csv", header = TRUE, stringsAsFactors = F)
displayDigit(train[9,-1])
```
Now, let’s preprocess the data by removing near zero variance columns and scaling by max(X). The data is also splitted into two for cross validation. Once again, we need to create a Y matrix with dimension N by K. This time the non-zero index in each row is offset by 1: label 0 will have entry 1 at index 1, label 1 will have entry 1 at index 2, and so on. In the end, we need to convert it back. (Another way is put 0 at index 10 and no offset for the rest labels.)
```{r}
nzv <- nearZeroVar(train)
nzv.nolabel <- nzv-1

inTrain <- createDataPartition(y=train$label, p=0.7, list=F)

training <- train[inTrain, ]
CV <- train[-inTrain, ]
X <- as.matrix(training[, -1]) # data matrix (each row = single example)
N <- nrow(X) # number of examples
y <- training[, 1] # class labels

K <- length(unique(y)) # number of classes
X.proc <- X[, -nzv.nolabel]/max(X) # scale
D <- ncol(X.proc) # dimensionality

Xcv <- as.matrix(CV[, -1]) # data matrix (each row = single example)
ycv <- CV[, 1] # class labels
Xcv.proc <- Xcv[, -nzv.nolabel]/max(X) # scale CV data

Y <- matrix(0, N, K)

for (i in 1:N){
  Y[i, y[i]+1] <- 1
}
```
**Model Training and Accuracy**
Now we can train the model with the training set. Note even after removing nzv columns, the data is still huge, so it may take a while for result to converge. Here I am only training the model for 3500 interations. You can vary the interations, learning rate and regularization strength and plot the learning curve for optimal fitting.
```{r}
nnet.mnist <- nnet(X.proc, Y, step_size = 0.3, reg = 0.0001, niteration = 3500)
predicted_class <- nnetPred(X.proc, nnet.mnist)
print(paste('training set accuracy:',mean(predicted_class == (y+1))))
predicted_class <- nnetPred(Xcv.proc, nnet.mnist)
print(paste('CV accuracy:',mean(predicted_class == (ycv+1))))
```

***prediction of a random image***
```{r}
Xtest <- Xcv[sample(1:nrow(Xcv), 1), ]
Xtest.proc <- as.matrix(Xtest[-nzv.nolabel], nrow = 1)
predicted_test <- nnetPred(t(Xtest.proc), nnet.mnist)
print(paste('The predicted digit is:',predicted_test-1 ))
displayDigit(Xtest)
```

accuracy can be improved by using better implementations from neural net and H2o, neural network etc.. packages



### Using ***neuralnet*** package

neural net package uses resilient backpropagatiion algorithm as opposed to normal backpropagation. difference can be observed in the code stub below
### Psedo code for resilient back propagation as opposed to normal backpropagation
```{r}
# for all weights{
# if (grad.old*grad>0){
# delta := min(delta*eta.plus, delta.max)
# weights := weights - sign(grad)*delta
# grad.old := grad
# }
# else if (grad.old*grad<0){
# weights := weights + sign(grad.old)*delta
# delta := max(delta*eta.minus, delta.min)
# grad.old := 0
# }
# else if (grad.old*grad=0){
# weights := weights - sign(grad)*delta
# grad.old := grad
# }
# }
# # weights for normal backpropagation
# for all weights{
# weights := weights - grad*delta
# }
```

**training options** when using ***neuralnet*** package
except for formula and data everything has default values 
1. ***formula***
in the form of y~x1+x2+...+xn
2. ***data***
matrix consisting of covariates and response in the formula
3. ***hidden***
***1*** list of neurons in each layer
4. ***threshold***
***0.01*** the threshhold for absolute value of gradient used to stop learning
5. ***rep***
***1*** no of repetitions the training procedure is repeated
6. ***startweights***
***rnorm()*** matrix of starting weights
7. ***algorithm***
***rprop+*** *backprop*:for gradient descent back propagation , *rprop+,rprp-* for resilient gradient descent with backtracking and without. *sad,slr* smallest absolute derivative and smallest learning rate
8. ***err.fct***
***sse*** the error function. "ce" fr common entropy, likelihood etc can be used
10. ***act.function***
***logistic*** .The activation function , note that sigmoid will give values between 0 and 1, tahn between -1 and 1
9. ***linear.output***
***T*** whether activation function should not be applied to output layer
11. ***likelihood***
***False*** indicator if likelihood is the error function, will calculate AIC, BIC if true
12. ***exclude***
***NULL*** matrix of weights that should be exlcuded from training
13. ***constant.weights***
***NULL*** values of weights to be excluded from training

**output**
All the info need to reproduce the nnet
1. ***net.result***
used to compute the result on new covariates
2. ***weights***
the fitted weights
3. ***generalisedweights***
the effect of ith covariate on log odds, similar to regression coefficient
4. ***result.matrix***
all the info of neural net
5. ***start.weights***
start weights used
6. ***covariate,resposne,data***
corresponding training data used

### Neural Nets Net for regression on Boston data

**Prepearing data for neuralnets**
```{r}
# check for missing values
library(MASS)
sum(is.na(Boston))

#split train and test
set.seed(500)
trainIndex = sample(nrow(Boston),round(0.75*nrow(Boston)))
```
**trying out neural networks before scalinig**
```{r}
Boston.train = Boston[trainIndex,]
Boston.test = Boston[-trainIndex,]
library(neuralnet)
#nnet.boston = neuralnet(formula = medv~.,data = Boston.train,hidden = c(5,3),linear.output = T)
# note that neural net function does not accept . in the formula
f = as.formula(paste("medv ~",paste(names(Boston.train)[1:13],collapse = "+")))
nnet.boston = neuralnet(formula = f,data = Boston.train,hidden = c(5,3),linear.output = T)
# calculate testMSE
predBoston = compute(nnet.boston,Boston.test[,1:13])$net.result
max(predBoston)
testBoston <- Boston.test$medv
max(testBoston)
mean((testBoston - predBoston)^2)
```
we see that the mean squared error is 72.2
**linear.output = T** as it is regression, for classification its false
check nn after scaling the data
```{r}
# scale the data
scaledBoston = as.data.frame(scale(Boston,center = T,scale = T))

Boston.train = scaledBoston[trainIndex,]
Boston.test = scaledBoston[-trainIndex,]
nnet.boston = neuralnet(formula = f,data = Boston.train,hidden = c(5,3),linear.output = T)

#plot the nn
plot(nnet.boston)
# shows bias yerms in blue and weights in black
gwplot(nnet.boston)


# calculate testMSE
predBoston = compute(nnet.boston,Boston.test[,1:13])$net.result

# Descaling for comparison
predBoston <- predBoston*sqrt(var(Boston$medv))+mean(Boston$medv)
testBoston <- Boston.test$medv*sqrt(var(Boston$medv))+mean(Boston$medv)

# Calculating MSE
MSE <- mean((testBoston - predBoston)^2)
MSE
```
so scaling improved the neuralnet, in general scaling makes weights on the bottom layer equally which can lead to better fitting. sometimes by not scaling the algorithm maynot even converge before reaching maximum number of iterations

Trying min,max scaling instead of mean,SD
```{r}
# Scaling data for the NN
maxs = apply(Boston, 2, max) 
mins = apply(Boston, 2, min)
scaledBoston = as.data.frame(scale(Boston, center = mins, scale = maxs - mins))
Boston.train = scaledBoston[trainIndex,]
Boston.test = scaledBoston[-trainIndex,]
nnet.boston = neuralnet(formula = f,data = Boston.train,hidden = c(5,3),linear.output = T)

# Calculating MSE
predBoston = compute(nnet.boston,Boston.test[,1:13])$net.result
predBoston = predBoston*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
testBoston = Boston.test$medv*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
MSE <- sum((testBoston - predBoston)^2)/nrow(Boston.test)
MSE

## plot prediction
plot(testBoston,predBoston,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
```


### Neural nets for classification on Infertility data
```{r}
library(datasets)
head(infert)
nn <- neuralnet(case~age+parity+induced+spontaneous,data=infert, hidden=2, err.fct="ce",linear.output=FALSE)
out <- cbind(nn$covariate,nn$net.result[[1]])
dimnames(out) <- list(NULL, c("age", "parity","induced","spontaneous","nn-output"))
head(out)
plot(nn)
# plot generalised weights
gwplot(nn,selected.covariate = "age",min=-2.5 , max = 5)
gwplot(nn,selected.covariate = "parity",min=-2.5 , max = 5)
gwplot(nn,selected.covariate = "induced",min=-2.5 , max = 5)
gwplot(nn,selected.covariate = "spontaneous",min=-2.5 , max = 5)
```
generalised weights form multivariate normal distribution . from the graph we can see that generalised weight of age is almost zero, and variance of induced and spontaneous is greater than one showing non linearity.
*confidence.interval* is a useful function if it satisfy conditions that 1. no irrelevant neurons 2. error function is log-likelihood
```{r}
nn <- neuralnet(case~parity+induced+spontaneous,data=infert, hidden=2, err.fct="ce",linear.output=FALSE,likelihood = T)
confidence.interval(nn,alpha = 0.05)
# prediction function
prediction(nn)
```

### Neural net to find XOR
```{r}
XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")
```

### Neural Net to find e^x

```{r}
traininginput <-  as.data.frame(runif(50, min=0, max=100))
trainingoutput <- e^(traininginput)
 
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
 
#Train the neural network

net.epower <- neuralnet(Output~Input,trainingdata, hidden=10, threshold=0.01)
print(net.epower)
 
#Plot the neural network
plot(net.epower)
 
#Test the neural network on some training data
testdata <- as.data.frame((1:10)^2) #Generate some squared numbers
net.results <- compute(net.epower, testdata) #Run them through the neural network
 
#Lets see what properties net.sqrt has
ls(net.results)
 
#Lets see the results
print(net.results$net.result)

cleanoutput <- cbind(testdata,e^(testdata),
                         as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
```
