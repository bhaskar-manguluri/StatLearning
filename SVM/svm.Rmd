---
title: "SVM"
author: "bhaskar"
output: html_document
---

Using library *e1071* for SVM
svm()
- kerner="linear"
- cost = cost of violation to the margin. if cost is large then narrow margnis and few support vectors violating
```{r}
library(e1071)
set.seed(10111)
x = matrix(rnorm(20*2),ncol = 2)
y = rep(c(-1,1),c(10,10))
x[y==1,] = x[y==1,]+1
plot(x,col=3-y,pch = 19)
dat = data.frame(x = x, y= as.factor(y))
svmfit = svm(y~.,data = dat,kernel = "linear",cost = 10,scale = F)
```
depending on the application, scaling can be True or false
```{r}
plot(svmfit,data = dat)
```
indices of support vectors can be diplayed by **index** column in the output
```{r}
svmfit$index
summary(svmfit)
dat[svmfit$index,]
```
*cost* is a tuning parameter for the svm(), ususally decided through *crossvalidation*
svm for above data using smaller cost
```{r}
svmfit1 = svm(y~.,data = dat,kernel = "linear",cost = 1,scale = F)
plot(svmfit1,data = dat)
svmfit1$index
summary(svmfit1)
```

SVM neither outputs the coefficients of linear boundary nor the width of the margin, cross validation is done using the tune() of e1071 library
```{r}
set.seed(1)
tune_output = tune(svm,y~.,data = dat,kernel="linear",ranges = list(cost=c(0.001,0.01,0.1,1.5,10,100,1000,1000)))
summary(tune_output)
#best model summary stored in best model column of the output
summary(tune_output$best.model)
plot(tune_output$best.model,data=dat)
```
check performance on a test set
```{r}
set.seed(10)
xtest = matrix(rnorm(20*2),ncol = 2)
ytest = rep(c(-1,1),c(10,10))
xtest[ytest==1,] = xtest[ytest==1,]+1
test_data = data.frame(x=xtest,y=as.factor(ytest))
yhat_predict = predict(tune_output$best.model,test_data)
table(actual=test_data$y,predicted = yhat_predict)
```
4 observation's are miscassified
checking the performance of svmfit with cost =10 as it is almost similar to tune.ot best model, but with lower cost
```{r}
yhat_predict = predict(svmfit,test_data)
table(actual=test_data$y,predicted = yhat_predict)
```
This performed slightly betteron this pirticular test set

What if our train set has clear boundary

```{r}
x[y==1,] = x[y==1]+0.5    #  increase the gap
plot(x,col=3-y,pch=19)
dat = data.frame(x = x, y= as.factor(y))
svm_fit2 = svm(y~.,data = dat,cost = 100,scale = F)
table(predict(svm_fit2,test_data),test_data$y)
tune_output = tune(svm,y~.,data = dat,kernel="linear",ranges = list(cost=c(0.001,0.01,0.1,1,5,10,100,1000,10000)))
summary(tune_output)
yhat = predict(tune_output$best.model,test_data)
table(yhat,test_data$y)
```
How will this perform on a clearly seperable test set
```{r}
xtest[ytest==1,] = xtest[ytest==1]+2    #  increase the gap
plot(xtest,col=3-ytest,pch=19)
test_data = data.frame(x=xtest,y=as.factor(ytest))
yhat = predict(svm_fit2,newdata = test_data)
table(actual=test_data$y,predicted=yhat)
```

svm with kernel="polynomial"/"radial" is non linear
```{r}
set.seed(10)
x = matrix(rnorm(200*2),ncol = 2)
y = rep(c(1,2),c(150,50))
x[1:100,] = x[1:100,]+2
x[101:150,]=x[101:150,]-2
plot(x,col=3-y,pch=19)
dat = data.frame(x = x,y=as.factor(y))
set.seed(10)
train = sample(200,100)
svm_radial = svm(y~.,kernel="radial",cost =0.5,gamma= 3,scale = F,data = dat[train,])
plot(svm_radial,data=dat[train,])
table(predict(svm_radial,newdata = dat[-train,]),dat[-train,]$y)
#tune for gamma and cost
tune_output = tune(svm,y~.,data = dat[train,],kernel="radial",ranges = list(cost=c(0.01,0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune_output)
plot(tune_output$best.model,data = dat[train,])
table(predict=predict(tune_output$best.model,newdata = dat[-train,]),actual=dat[-train,]$y)
```
**ROC plots**

```{r}
library(ROCR)
rocplot = function(pred,thruth,...){
        predob = prediction(pred,thruth)
        perf = performance(predob,"tpr","fpr")
        plot(perf,...)
}
```
By default ***svm*** function outputs class labels, if we want fitted values instead argument ***decision.values=T***
```{r}
svmfit = svm(y~., data = dat[train,],kernel="radial",gamma=2,cost=1,decision.values=T)
yhat = predict(svmfit,newdata=dat[train,],decision.values= T)
fitted = attributes(yhat)$decision.values
rocplot(fitted,dat[train,"y"],main="ROC on Train data")
ModelMetrics::auc(actual = dat[train,"y"],predicted = fitted)
```
***svm*** produces better flexible fit with higher value of gamma
```{r}
svmfit = svm(y~., data = dat[train,],kernel="radial",gamma=50,cost=1,decision.values=T)
yhat = predict(svmfit,newdata=dat[train,],decision.values= T)
fitted = attributes(yhat)$decision.values
rocplot(fitted,dat[train,"y"],main="ROC on Train data")
ModelMetrics::auc(actual = dat[train,"y"],predicted = fitted)
```
But ***ROC curve performs better with gamma =2 for test data
```{r}
yhat = predict(svmfit,newdata=dat[-train,],decision.values= T)
fitted = attributes(yhat)$decision.values
rocplot(fitted,dat[-train,"y"],main="ROC on Train data")
ModelMetrics::auc(actual = dat[train,"y"],predicted = fitted)
```

**SVM for multiple classes**
performing svm on previous data by adding a third class, ***svm*** function performs one vs one apprach for the dataset
```{r}
set.seed(1)
x= rbind(x,matrix(rnorm(50*2),ncol=2))
y = c(y,rep(0,50))
x[y %in% c(0,2)]=x[y %in% c(0,2)]+2
dat = data.frame(x,as.factor(y))
svm_fit_multiclass = svm(y~.,data = dat,kernel="radial",gamma=1,cost=1)
summary(svm_fit_multiclass)
```
```{r}
plot(svm_fit_multiclass,dat)
```


### Application to gene expression data

For ***Khan data set*** which consists of gene expression measurement of several tissues of four distinct types of small round blue tumors
```{r}
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
```

In this example we have very wide data set with 6308 columns per 63 observations , hence using ***linear kernal*** would be sufficient, as added flexibility using a polynaomial and radial kernel is unnecessary

```{r}
dat = data.frame(x=Khan$xtrain,y=as.factor(Khan$ytrain))
svm.model = svm(y~.,data=dat,kernel="linear",cost = 10)
table(svm.model$fitted,dat$y)
dat.te = data.frame(x=Khan$xtest,y=as.factor(Khan$ytest))
table(pred=predict(svm.model,newdata = dat.te),thruth=dat.te$y)
```
We see that using cost=10 yields in 2 test set errors
